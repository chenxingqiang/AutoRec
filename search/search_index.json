{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AutoRec Abstract Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec , an open-source automated machine learning (AutoML) platform extended from the TensorFlow ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge.","title":"Home"},{"location":"#welcome-to-autorec","text":"","title":"Welcome to AutoRec"},{"location":"#abstract","text":"Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec , an open-source automated machine learning (AutoML) platform extended from the TensorFlow ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge.","title":"Abstract"},{"location":"about/","text":"This package is developed by DATA LAB at Texas A&M University. Core Team Ting-Hsiang Wang : Qingquan Song : Xiaotian Han : Zirui Liu : Haifeng Jin : Xia \"Ben\" Hu : Project lead and maintainer.","title":"About"},{"location":"about/#core-team","text":"Ting-Hsiang Wang : Qingquan Song : Xiaotian Han : Zirui Liu : Haifeng Jin : Xia \"Ben\" Hu : Project lead and maintainer.","title":"Core Team"},{"location":"auto_search/","text":"[source] Search autorecsys . auto_search . Search ( model = None , name = None , tuner = \"random\" , tuner_params = None , directory = \".\" , overwrite = True ) A search object to search on a Recommender HyperModel (CTRRecommender/RPRecommender) defined by inputs and outputs. Search combines a Recommender and a Tuner to tune the Recommender. The user can use search() to perform search, and use a similar way to a Keras model to adopt the best discovered model as it also has fit() / predict() / evaluate() methods. The user should input a Recommender HyperModel (CTRRecommender/RPRecommender) and a selected tuning method to initial the Search object and input the dataset when calling the search method to discover the best architecture. # Arguments model: A Recommender HyperModel (CTRRecommender/RPRecommender). name: String. The name of the project, which is used for saving and loading purposes. tuner: String. The name of the tuner. It should be one of 'greedy', 'bayesian' or 'random'. Default to be 'random'. tuner_params: Dict. The hyperparameters of the tuner. The commons ones are: 'max_trials': Int. Specify the number of search epochs. 'overwrite': Boolean. Whether we want to ovewrite an existing tuner or not. directory: String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the project in the current directory, i.e., ``directory/name``. overwrite: Boolean. Defaults to `True`. Whether we want to ovewrite an existing project with the name defined as ``directory/name`` or not. ---- <span style=\"float:right;\">[[source]](https://github.com/datamllab/AutoRecSys/autorecsys/auto_search.py#L67)</span> ### search ```python Search.search(x=None, y=None, x_val=None, y_val=None, objective=\"mse\", **fit_kwargs) Search the best deep recommendation model. Arguments x : numpy array. Training features. y : numpy array. Training targets. x_val : numpy array. Validation features. y_val : numpy array. Validation features. objective : String. Name of model metric to minimize or maximize, e.g. 'val_BinaryCrossentropy'. Defaults to 'mse'. **fit_kwargs : Any arguments supported by the fit method of a Keras model such as: batch_size , epochs , callbacks . [source] predict Search . predict ( x ) Use the best searched model to conduct prediction on the dataset x. Arguments x : numpy array / data frame / string path of a csv file. Features used to do the prediction. [source] evaluate Search . evaluate ( x , y_true ) Evaluate the best searched model. Arguments x : numpy array / data frame / string path of a csv file. Features used to do the prediction. y_true : numpy array / data frame / string path of a csv file. Ground-truth labels.","title":"Auto Search"},{"location":"auto_search/#search","text":"autorecsys . auto_search . Search ( model = None , name = None , tuner = \"random\" , tuner_params = None , directory = \".\" , overwrite = True ) A search object to search on a Recommender HyperModel (CTRRecommender/RPRecommender) defined by inputs and outputs. Search combines a Recommender and a Tuner to tune the Recommender. The user can use search() to perform search, and use a similar way to a Keras model to adopt the best discovered model as it also has fit() / predict() / evaluate() methods. The user should input a Recommender HyperModel (CTRRecommender/RPRecommender) and a selected tuning method to initial the Search object and input the dataset when calling the search method to discover the best architecture. # Arguments model: A Recommender HyperModel (CTRRecommender/RPRecommender). name: String. The name of the project, which is used for saving and loading purposes. tuner: String. The name of the tuner. It should be one of 'greedy', 'bayesian' or 'random'. Default to be 'random'. tuner_params: Dict. The hyperparameters of the tuner. The commons ones are: 'max_trials': Int. Specify the number of search epochs. 'overwrite': Boolean. Whether we want to ovewrite an existing tuner or not. directory: String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the project in the current directory, i.e., ``directory/name``. overwrite: Boolean. Defaults to `True`. Whether we want to ovewrite an existing project with the name defined as ``directory/name`` or not. ---- <span style=\"float:right;\">[[source]](https://github.com/datamllab/AutoRecSys/autorecsys/auto_search.py#L67)</span> ### search ```python Search.search(x=None, y=None, x_val=None, y_val=None, objective=\"mse\", **fit_kwargs) Search the best deep recommendation model. Arguments x : numpy array. Training features. y : numpy array. Training targets. x_val : numpy array. Validation features. y_val : numpy array. Validation features. objective : String. Name of model metric to minimize or maximize, e.g. 'val_BinaryCrossentropy'. Defaults to 'mse'. **fit_kwargs : Any arguments supported by the fit method of a Keras model such as: batch_size , epochs , callbacks . [source]","title":"Search"},{"location":"auto_search/#predict","text":"Search . predict ( x ) Use the best searched model to conduct prediction on the dataset x. Arguments x : numpy array / data frame / string path of a csv file. Features used to do the prediction. [source]","title":"predict"},{"location":"auto_search/#evaluate","text":"Search . evaluate ( x , y_true ) Evaluate the best searched model. Arguments x : numpy array / data frame / string path of a csv file. Features used to do the prediction. y_true : numpy array / data frame / string path of a csv file. Ground-truth labels.","title":"evaluate"},{"location":"benchmark/","text":"","title":"Benchmark"},{"location":"install/","text":"Requirements Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.2.0 : AutoRec is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup. Install AutoRec","title":"Installation"},{"location":"install/#requirements","text":"Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.2.0 : AutoRec is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup.","title":"Requirements"},{"location":"install/#install-autorec","text":"","title":"Install AutoRec"},{"location":"interactor/","text":"[source] RandomSelectInteraction autorecsys . pipeline . interactor . RandomSelectInteraction ( ** kwargs ) Module for output one vector select form the input vector list . Attributes: None [source] get_state RandomSelectInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state RandomSelectInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build RandomSelectInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] ConcatenateInteraction autorecsys . pipeline . interactor . ConcatenateInteraction ( ** kwargs ) Module for outputing one vector by concatenating the input vector list. Attributes: None [source] get_state ConcatenateInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state ConcatenateInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build ConcatenateInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] ElementwiseInteraction autorecsys . pipeline . interactor . ElementwiseInteraction ( elementwise_type = None , ** kwargs ) Module for element-wise operation. this block includes the element-wise sum ,average, innerporduct, max, and min. The default operation is average. Attributes: elementwise_type(\"str\"): Can be used to select the element-wise operation. the default value is None. If the value of this parameter is None, the block can select the operation for the sum ,average, innerporduct, max, and min, according to the search algorithm. [source] get_state ElementwiseInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state ElementwiseInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build ElementwiseInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] MLPInteraction autorecsys . pipeline . interactor . MLPInteraction ( units = None , num_layers = None , use_batchnorm = None , dropout_rate = None , ** kwargs ) Module for MLP operation. This block can seted as MLP with different layer, unit, and other setting . Attributes: units (int). The units of all layer in the MLP block. num_layers (int). The number of the layers in the MLP blck use_batchnorm (Boolean). Use batch normalization or not. dropout_rate(float). The value of drop out in the last layer of MLP. [source] get_state MLPInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state MLPInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build MLPInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] HyperInteraction autorecsys . pipeline . interactor . HyperInteraction ( meta_interator_num = None , interactor_type = None , ** kwargs ) Module for selecting different block. This block includes can select different blocks in the interactor. Attributes: meta_interator_num (str). The total number of the meta interoctor block. interactor_type (str). The type of interactor used in this block. [source] get_state HyperInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state HyperInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build HyperInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] FMInteraction autorecsys . pipeline . interactor . FMInteraction ( embedding_dim = None , ** kwargs ) CTR module for factorization machine operation. Reference: https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf This block applies factorization machine operation on a list of input 3D tensors of size (batch_size, field_size, embedding_size). It will align the dimension of tensors to 3D if they're 1D or 2D originally, and will align/transfrom the last embedding dimension based on a tunable hyperaparmeter. Attributes: embedding_dim (int). The transformed embedding dimension of each field, before conducting the factorization machine operation. [source] get_state FMInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state FMInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build FMInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] CrossNetInteraction autorecsys . pipeline . interactor . CrossNetInteraction ( layer_num = None , ** kwargs ) CTR module for crossnet layer in deep & cross network. Reference: https://arxiv.org/pdf/1708.05123.pdf This block applies cross interaction operation on a 2D tensors of size (batch_size, embedding_size). We assume the input could be a list of tensors of 2D or 3D, and the block will flatten them as as list of 2D tensors, and ten concatenate them as a single 2D tensor. The cross interaction follows the reference and the number of layers of the cross interaction is tunable. Attributes: layer_num (int). The number of layers of the cross interaction. [source] get_state CrossNetInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state CrossNetInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build CrossNetInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] SelfAttentionInteraction autorecsys . pipeline . interactor . SelfAttentionInteraction ( embedding_dim = None , att_embedding_dim = None , head_num = None , residual = None , ** kwargs ) CTR module for the multi-head self-attention layer in the autoint paper. Reference: https://arxiv.org/pdf/1708.05123.pdf This block applies multi-head self-attention on a 3D tensor of size (batch_size, field_size, embedding_size). We assume the input could be a list of tensors of 1D, 2D or 3D, and the block will align the dimension of tensors to 3D if they're 1D or 2D originally, and it will also align the last embedding dimension based on a tunable hyperaparmeter. Attributes: embedding_dim (int). Embedding dimension for aligning embedding dimension of the input tensors. att_embedding_dim (int). Output embedding dimension after the mulit-head self-attention. head_num (int). Number of attention heads. residual (boolean). Whether to apply residual connection after self-attention or not. [source] get_state SelfAttentionInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state SelfAttentionInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build SelfAttentionInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"Interactor"},{"location":"interactor/#randomselectinteraction","text":"autorecsys . pipeline . interactor . RandomSelectInteraction ( ** kwargs ) Module for output one vector select form the input vector list . Attributes: None [source]","title":"RandomSelectInteraction"},{"location":"interactor/#get_state","text":"RandomSelectInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state","text":"RandomSelectInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build","text":"RandomSelectInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#concatenateinteraction","text":"autorecsys . pipeline . interactor . ConcatenateInteraction ( ** kwargs ) Module for outputing one vector by concatenating the input vector list. Attributes: None [source]","title":"ConcatenateInteraction"},{"location":"interactor/#get_state_1","text":"ConcatenateInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_1","text":"ConcatenateInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_1","text":"ConcatenateInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#elementwiseinteraction","text":"autorecsys . pipeline . interactor . ElementwiseInteraction ( elementwise_type = None , ** kwargs ) Module for element-wise operation. this block includes the element-wise sum ,average, innerporduct, max, and min. The default operation is average. Attributes: elementwise_type(\"str\"): Can be used to select the element-wise operation. the default value is None. If the value of this parameter is None, the block can select the operation for the sum ,average, innerporduct, max, and min, according to the search algorithm. [source]","title":"ElementwiseInteraction"},{"location":"interactor/#get_state_2","text":"ElementwiseInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_2","text":"ElementwiseInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_2","text":"ElementwiseInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#mlpinteraction","text":"autorecsys . pipeline . interactor . MLPInteraction ( units = None , num_layers = None , use_batchnorm = None , dropout_rate = None , ** kwargs ) Module for MLP operation. This block can seted as MLP with different layer, unit, and other setting . Attributes: units (int). The units of all layer in the MLP block. num_layers (int). The number of the layers in the MLP blck use_batchnorm (Boolean). Use batch normalization or not. dropout_rate(float). The value of drop out in the last layer of MLP. [source]","title":"MLPInteraction"},{"location":"interactor/#get_state_3","text":"MLPInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_3","text":"MLPInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_3","text":"MLPInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#hyperinteraction","text":"autorecsys . pipeline . interactor . HyperInteraction ( meta_interator_num = None , interactor_type = None , ** kwargs ) Module for selecting different block. This block includes can select different blocks in the interactor. Attributes: meta_interator_num (str). The total number of the meta interoctor block. interactor_type (str). The type of interactor used in this block. [source]","title":"HyperInteraction"},{"location":"interactor/#get_state_4","text":"HyperInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_4","text":"HyperInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_4","text":"HyperInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#fminteraction","text":"autorecsys . pipeline . interactor . FMInteraction ( embedding_dim = None , ** kwargs ) CTR module for factorization machine operation. Reference: https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf This block applies factorization machine operation on a list of input 3D tensors of size (batch_size, field_size, embedding_size). It will align the dimension of tensors to 3D if they're 1D or 2D originally, and will align/transfrom the last embedding dimension based on a tunable hyperaparmeter. Attributes: embedding_dim (int). The transformed embedding dimension of each field, before conducting the factorization machine operation. [source]","title":"FMInteraction"},{"location":"interactor/#get_state_5","text":"FMInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_5","text":"FMInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_5","text":"FMInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#crossnetinteraction","text":"autorecsys . pipeline . interactor . CrossNetInteraction ( layer_num = None , ** kwargs ) CTR module for crossnet layer in deep & cross network. Reference: https://arxiv.org/pdf/1708.05123.pdf This block applies cross interaction operation on a 2D tensors of size (batch_size, embedding_size). We assume the input could be a list of tensors of 2D or 3D, and the block will flatten them as as list of 2D tensors, and ten concatenate them as a single 2D tensor. The cross interaction follows the reference and the number of layers of the cross interaction is tunable. Attributes: layer_num (int). The number of layers of the cross interaction. [source]","title":"CrossNetInteraction"},{"location":"interactor/#get_state_6","text":"CrossNetInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_6","text":"CrossNetInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_6","text":"CrossNetInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"interactor/#selfattentioninteraction","text":"autorecsys . pipeline . interactor . SelfAttentionInteraction ( embedding_dim = None , att_embedding_dim = None , head_num = None , residual = None , ** kwargs ) CTR module for the multi-head self-attention layer in the autoint paper. Reference: https://arxiv.org/pdf/1708.05123.pdf This block applies multi-head self-attention on a 3D tensor of size (batch_size, field_size, embedding_size). We assume the input could be a list of tensors of 1D, 2D or 3D, and the block will align the dimension of tensors to 3D if they're 1D or 2D originally, and it will also align the last embedding dimension based on a tunable hyperaparmeter. Attributes: embedding_dim (int). Embedding dimension for aligning embedding dimension of the input tensors. att_embedding_dim (int). Output embedding dimension after the mulit-head self-attention. head_num (int). Number of attention heads. residual (boolean). Whether to apply residual connection after self-attention or not. [source]","title":"SelfAttentionInteraction"},{"location":"interactor/#get_state_7","text":"SelfAttentionInteraction . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"interactor/#set_state_7","text":"SelfAttentionInteraction . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"interactor/#build_7","text":"SelfAttentionInteraction . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"build"},{"location":"mapper/","text":"[source] LatentFactorMapper autorecsys . pipeline . mapper . LatentFactorMapper ( feat_column_id = None , id_num = None , embedding_dim = None , ** kwargs ) Module for mapping the user/item id to the laten factor. Attributes: feat_column_id (int): the id of the used feateure. id_num (int): The total number of the user/item id. embedding_dim (int): The embedding size of the latent factor. [source] get_state LatentFactorMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state LatentFactorMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build LatentFactorMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] DenseFeatureMapper autorecsys . pipeline . mapper . DenseFeatureMapper ( num_of_fields = None , embedding_dim = None , ** kwargs ) Mapper for dense feature that can map dense feature to embedding. Attributes: num_of_fields (int): the number of feature fields. embedding_dim (int): The embedding size of the latent factor. [source] get_state DenseFeatureMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state DenseFeatureMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build DenseFeatureMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] SparseFeatureMapper autorecsys . pipeline . mapper . SparseFeatureMapper ( num_of_fields = None , hash_size = None , embedding_dim = None , ** kwargs ) Mapper for sparse feature that can map categorical features to embedding. Attributes: num_of_fields (int): the number of feature fields. hash_size (int): size for every feature. embedding_dim (int): The embedding size of the latent factor. [source] get_state SparseFeatureMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source] set_state SparseFeatureMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source] build SparseFeatureMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"Mapper"},{"location":"mapper/#latentfactormapper","text":"autorecsys . pipeline . mapper . LatentFactorMapper ( feat_column_id = None , id_num = None , embedding_dim = None , ** kwargs ) Module for mapping the user/item id to the laten factor. Attributes: feat_column_id (int): the id of the used feateure. id_num (int): The total number of the user/item id. embedding_dim (int): The embedding size of the latent factor. [source]","title":"LatentFactorMapper"},{"location":"mapper/#get_state","text":"LatentFactorMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"mapper/#set_state","text":"LatentFactorMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"mapper/#build","text":"LatentFactorMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"mapper/#densefeaturemapper","text":"autorecsys . pipeline . mapper . DenseFeatureMapper ( num_of_fields = None , embedding_dim = None , ** kwargs ) Mapper for dense feature that can map dense feature to embedding. Attributes: num_of_fields (int): the number of feature fields. embedding_dim (int): The embedding size of the latent factor. [source]","title":"DenseFeatureMapper"},{"location":"mapper/#get_state_1","text":"DenseFeatureMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"mapper/#set_state_1","text":"DenseFeatureMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"mapper/#build_1","text":"DenseFeatureMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"mapper/#sparsefeaturemapper","text":"autorecsys . pipeline . mapper . SparseFeatureMapper ( num_of_fields = None , hash_size = None , embedding_dim = None , ** kwargs ) Mapper for sparse feature that can map categorical features to embedding. Attributes: num_of_fields (int): the number of feature fields. hash_size (int): size for every feature. embedding_dim (int): The embedding size of the latent factor. [source]","title":"SparseFeatureMapper"},{"location":"mapper/#get_state_2","text":"SparseFeatureMapper . get_state () Get the configuration of the preprocessor. Returns A dictionary of configurations of the preprocessor. [source]","title":"get_state"},{"location":"mapper/#set_state_2","text":"SparseFeatureMapper . set_state ( state ) Set the configuration of the preprocessor. Arguments state : A dictionary of the configurations of the preprocessor. [source]","title":"set_state"},{"location":"mapper/#build_2","text":"SparseFeatureMapper . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"build"},{"location":"node/","text":"[source] Input autorecsys . pipeline . node . Input ( shape = None ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. [source] fit_transform Input . fit_transform ( x ) [source] transform Input . transform ( x ) Transform x into a compatible type (tf.data.Dataset). [source] StructuredDataInput autorecsys . pipeline . node . StructuredDataInput ( column_names = None , column_types = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. [source] get_state StructuredDataInput . get_state () [source] set_state StructuredDataInput . set_state ( state ) [source] update StructuredDataInput . update ( x ) [source] infer_column_types StructuredDataInput . infer_column_types ()","title":"Node"},{"location":"node/#input","text":"autorecsys . pipeline . node . Input ( shape = None ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. [source]","title":"Input"},{"location":"node/#fit_transform","text":"Input . fit_transform ( x ) [source]","title":"fit_transform"},{"location":"node/#transform","text":"Input . transform ( x ) Transform x into a compatible type (tf.data.Dataset). [source]","title":"transform"},{"location":"node/#structureddatainput","text":"autorecsys . pipeline . node . StructuredDataInput ( column_names = None , column_types = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Arguments column_names : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. [source]","title":"StructuredDataInput"},{"location":"node/#get_state","text":"StructuredDataInput . get_state () [source]","title":"get_state"},{"location":"node/#set_state","text":"StructuredDataInput . set_state ( state ) [source]","title":"set_state"},{"location":"node/#update","text":"StructuredDataInput . update ( x ) [source]","title":"update"},{"location":"node/#infer_column_types","text":"StructuredDataInput . infer_column_types ()","title":"infer_column_types"},{"location":"optimizer/","text":"[source] RatingPredictionOptimizer autorecsys . pipeline . optimizer . RatingPredictionOptimizer ( ** kwargs ) Module for rating prediction task. Attributes: None. [source] build RatingPredictionOptimizer . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source] PointWiseOptimizer autorecsys . pipeline . optimizer . PointWiseOptimizer ( name = None , ** kwargs ) Module for click through rate prediction. Attributes: None. [source] build PointWiseOptimizer . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"Optimizer"},{"location":"optimizer/#ratingpredictionoptimizer","text":"autorecsys . pipeline . optimizer . RatingPredictionOptimizer ( ** kwargs ) Module for rating prediction task. Attributes: None. [source]","title":"RatingPredictionOptimizer"},{"location":"optimizer/#build","text":"RatingPredictionOptimizer . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance. [source]","title":"build"},{"location":"optimizer/#pointwiseoptimizer","text":"autorecsys . pipeline . optimizer . PointWiseOptimizer ( name = None , ** kwargs ) Module for click through rate prediction. Attributes: None. [source]","title":"PointWiseOptimizer"},{"location":"optimizer/#build_1","text":"PointWiseOptimizer . build ( hp , inputs = None ) Builds a model. Arguments: hp: A HyperParameters instance. Returns: A model instance.","title":"build"},{"location":"preprocessor/","text":"[source] BasePreprocessor autorecsys . pipeline . preprocessor . BasePreprocessor ( non_csv_path = None , csv_path = None , header = None , columns = None , delimiter = None , filler = None , dtype_dict = None , ignored_columns = None , target_column = None , numerical_columns = None , categorical_columns = None , categorical_filter = None , fit_dictionary_path = None , transform_path = None , test_percentage = None , validate_percentage = None , train_path = None , validate_path = None , test_path = None , ) Preprocess data into Pandas DataFrame format. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. Attributes non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. data_df (DataFrame) : Data loaded in Pandas DataFrame format and contains only relevant columns. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dict (dict) : Map string categorical column names to dictionary which maps categories to indices. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] format_dataset BasePreprocessor . format_dataset () (Optional) Convert dataset into CSV format. Note User should implement this function to convert non-CSV dataset into CSV format. [source] load_dataset BasePreprocessor . load_dataset () Load CSV data as a Pandas DataFrame object. [source] transform_categorical BasePreprocessor . transform_categorical () Transform categorical data. Note Produce fit dictionary for categorical data and transform categorical data using fit dictionary. [source] transform_numerical BasePreprocessor . transform_numerical () Transform numerical data using supported data transformation functions. [source] get_hash_size BasePreprocessor . get_hash_size () Get the hash sizes of categorical columns. Returns List of integer numbers of categories in each categorical data columns. [source] get_x BasePreprocessor . get_x () Get the training data columns. Returns DataFrame training data columns. [source] get_x_numerical BasePreprocessor . get_x_numerical ( x ) Get the numerical columns from the input data columns. Arguments x (DataFrame) : The input data columns. Returns ndarray numerical columns in the input data columns. [source] get_x_categorical BasePreprocessor . get_x_categorical ( x ) Get the categorical columns from the input data columns. Arguments x (DataFrame) : The input data columns. Returns ndarray categorical columns in the input data columns. [source] get_y BasePreprocessor . get_y () Get the output column. Returns ndarray output column. [source] get_numerical_count BasePreprocessor . get_numerical_count () Get the number of numerical columns. Returns Integer number of numerical columns. [source] get_categorical_count BasePreprocessor . get_categorical_count () Get the number of categorical columns. Returns Integer number of categorical columns. [source] split_data BasePreprocessor . split_data ( x , y , test_percentage ) Split data into the train, validation, and test sets. Arguments x (ndarray) : (M, N) matrix associated with the numerical and categorical data, where M is the number of rows and N is the number of numerical and categorical columns. y (ndarray) : (M, 1) matrix associated with the label data, where M is the number of rows. test_percentage (float) : Percentage of test set. Returns 4-tuple of ndarray training input data, testing input data, training output data, and testing output data. [source] preprocess BasePreprocessor . preprocess () Apply all preprocess steps. Note User is responsible for calling the needed data preprocessing functions from here. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source] AvazuPreprocessor autorecsys . pipeline . preprocessor . AvazuPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/avazu/train-10k\" , header = 0 , columns = None , delimiter = \",\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"click\" , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Avazu dataset for click-through rate (CTR) prediction. Note: To obtain the Avazu dataset, visit: https://www.kaggle.com/c/avazu-ctr-prediction The Avazu dataset has 24 data columns: [0] is ignored, [1] is label, and [2-23] are categorical. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] preprocess AvazuPreprocessor . preprocess () Apply all preprocessing steps to the Avazu dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source] CriteoPreprocessor autorecsys . pipeline . preprocessor . CriteoPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/criteo/train-10k.txt\" , header = None , columns = None , delimiter = \" \\t \" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = 0 , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Criteo dataset for click-through rate (CTR) prediction. Note To obtain the Criteo dataset, visit: https://www.kaggle.com/c/criteo-display-ad-challenge/ The Criteo dataset has 40 data columns: [0] is label, [1-13] are numerical, and [14-39] are categorical. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] preprocess CriteoPreprocessor . preprocess () Apply all preprocessing steps to the Criteo dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source] NetflixPrizePreprocessor autorecsys . pipeline . preprocessor . NetflixPrizePreprocessor ( non_csv_path = \"./example_datasets/netflix/combined_data_1-10k.txt\" , csv_path = \"./example_datasets/netflix/combined_data_1-10k.csv\" , header = None , columns = None , delimiter = \",\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"Rating\" , numerical_columns = None , categorical_columns = None , categorical_filter = 0 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Netflix dataset for rating prediction. Note To obtain the Netflix dataset, visit: https://www.kaggle.com/netflix-inc/netflix-prize-data The Netflix dataset has 4 data columns: MovieID, CustomerID, Rating, and Date. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] format_dataset NetflixPrizePreprocessor . format_dataset () Convert the Netflix Prize dataset into CSV format and save it as a new file. Note: This is an example showing the function which converts dataset into the CSV format as provided by user. [source] preprocess NetflixPrizePreprocessor . preprocess () Apply all preprocessing steps to the Netflix Prize dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source] MovielensPreprocessor autorecsys . pipeline . preprocessor . MovielensPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/movielens/ratings-10k.dat\" , header = None , columns = None , delimiter = \"::\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"Rating\" , numerical_columns = None , categorical_columns = None , categorical_filter = 0 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Movielens 1M dataset for rating prediction. Note To obtain the Movielens 1M dataset, visit: https://grouplens.org/datasets/movielens/ The Movielens 1M dataset has 4 data columns: UserID, MovieID, Rating, and Timestamp. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source] preprocess MovielensPreprocessor . preprocess () Apply all preprocessing steps to the Movielens 1M dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data.","title":"Preprocessor"},{"location":"preprocessor/#basepreprocessor","text":"autorecsys . pipeline . preprocessor . BasePreprocessor ( non_csv_path = None , csv_path = None , header = None , columns = None , delimiter = None , filler = None , dtype_dict = None , ignored_columns = None , target_column = None , numerical_columns = None , categorical_columns = None , categorical_filter = None , fit_dictionary_path = None , transform_path = None , test_percentage = None , validate_percentage = None , train_path = None , validate_path = None , test_path = None , ) Preprocess data into Pandas DataFrame format. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. Attributes non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. data_df (DataFrame) : Data loaded in Pandas DataFrame format and contains only relevant columns. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dict (dict) : Map string categorical column names to dictionary which maps categories to indices. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"BasePreprocessor"},{"location":"preprocessor/#format_dataset","text":"BasePreprocessor . format_dataset () (Optional) Convert dataset into CSV format. Note User should implement this function to convert non-CSV dataset into CSV format. [source]","title":"format_dataset"},{"location":"preprocessor/#load_dataset","text":"BasePreprocessor . load_dataset () Load CSV data as a Pandas DataFrame object. [source]","title":"load_dataset"},{"location":"preprocessor/#transform_categorical","text":"BasePreprocessor . transform_categorical () Transform categorical data. Note Produce fit dictionary for categorical data and transform categorical data using fit dictionary. [source]","title":"transform_categorical"},{"location":"preprocessor/#transform_numerical","text":"BasePreprocessor . transform_numerical () Transform numerical data using supported data transformation functions. [source]","title":"transform_numerical"},{"location":"preprocessor/#get_hash_size","text":"BasePreprocessor . get_hash_size () Get the hash sizes of categorical columns. Returns List of integer numbers of categories in each categorical data columns. [source]","title":"get_hash_size"},{"location":"preprocessor/#get_x","text":"BasePreprocessor . get_x () Get the training data columns. Returns DataFrame training data columns. [source]","title":"get_x"},{"location":"preprocessor/#get_x_numerical","text":"BasePreprocessor . get_x_numerical ( x ) Get the numerical columns from the input data columns. Arguments x (DataFrame) : The input data columns. Returns ndarray numerical columns in the input data columns. [source]","title":"get_x_numerical"},{"location":"preprocessor/#get_x_categorical","text":"BasePreprocessor . get_x_categorical ( x ) Get the categorical columns from the input data columns. Arguments x (DataFrame) : The input data columns. Returns ndarray categorical columns in the input data columns. [source]","title":"get_x_categorical"},{"location":"preprocessor/#get_y","text":"BasePreprocessor . get_y () Get the output column. Returns ndarray output column. [source]","title":"get_y"},{"location":"preprocessor/#get_numerical_count","text":"BasePreprocessor . get_numerical_count () Get the number of numerical columns. Returns Integer number of numerical columns. [source]","title":"get_numerical_count"},{"location":"preprocessor/#get_categorical_count","text":"BasePreprocessor . get_categorical_count () Get the number of categorical columns. Returns Integer number of categorical columns. [source]","title":"get_categorical_count"},{"location":"preprocessor/#split_data","text":"BasePreprocessor . split_data ( x , y , test_percentage ) Split data into the train, validation, and test sets. Arguments x (ndarray) : (M, N) matrix associated with the numerical and categorical data, where M is the number of rows and N is the number of numerical and categorical columns. y (ndarray) : (M, 1) matrix associated with the label data, where M is the number of rows. test_percentage (float) : Percentage of test set. Returns 4-tuple of ndarray training input data, testing input data, training output data, and testing output data. [source]","title":"split_data"},{"location":"preprocessor/#preprocess","text":"BasePreprocessor . preprocess () Apply all preprocess steps. Note User is responsible for calling the needed data preprocessing functions from here. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source]","title":"preprocess"},{"location":"preprocessor/#avazupreprocessor","text":"autorecsys . pipeline . preprocessor . AvazuPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/avazu/train-10k\" , header = 0 , columns = None , delimiter = \",\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"click\" , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Avazu dataset for click-through rate (CTR) prediction. Note: To obtain the Avazu dataset, visit: https://www.kaggle.com/c/avazu-ctr-prediction The Avazu dataset has 24 data columns: [0] is ignored, [1] is label, and [2-23] are categorical. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"AvazuPreprocessor"},{"location":"preprocessor/#preprocess_1","text":"AvazuPreprocessor . preprocess () Apply all preprocessing steps to the Avazu dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source]","title":"preprocess"},{"location":"preprocessor/#criteopreprocessor","text":"autorecsys . pipeline . preprocessor . CriteoPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/criteo/train-10k.txt\" , header = None , columns = None , delimiter = \" \\t \" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = 0 , numerical_columns = None , categorical_columns = None , categorical_filter = 3 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Criteo dataset for click-through rate (CTR) prediction. Note To obtain the Criteo dataset, visit: https://www.kaggle.com/c/criteo-display-ad-challenge/ The Criteo dataset has 40 data columns: [0] is label, [1-13] are numerical, and [14-39] are categorical. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"CriteoPreprocessor"},{"location":"preprocessor/#preprocess_2","text":"CriteoPreprocessor . preprocess () Apply all preprocessing steps to the Criteo dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source]","title":"preprocess"},{"location":"preprocessor/#netflixprizepreprocessor","text":"autorecsys . pipeline . preprocessor . NetflixPrizePreprocessor ( non_csv_path = \"./example_datasets/netflix/combined_data_1-10k.txt\" , csv_path = \"./example_datasets/netflix/combined_data_1-10k.csv\" , header = None , columns = None , delimiter = \",\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"Rating\" , numerical_columns = None , categorical_columns = None , categorical_filter = 0 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Netflix dataset for rating prediction. Note To obtain the Netflix dataset, visit: https://www.kaggle.com/netflix-inc/netflix-prize-data The Netflix dataset has 4 data columns: MovieID, CustomerID, Rating, and Date. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"NetflixPrizePreprocessor"},{"location":"preprocessor/#format_dataset_1","text":"NetflixPrizePreprocessor . format_dataset () Convert the Netflix Prize dataset into CSV format and save it as a new file. Note: This is an example showing the function which converts dataset into the CSV format as provided by user. [source]","title":"format_dataset"},{"location":"preprocessor/#preprocess_3","text":"NetflixPrizePreprocessor . preprocess () Apply all preprocessing steps to the Netflix Prize dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data. [source]","title":"preprocess"},{"location":"preprocessor/#movielenspreprocessor","text":"autorecsys . pipeline . preprocessor . MovielensPreprocessor ( non_csv_path = None , csv_path = \"./example_datasets/movielens/ratings-10k.dat\" , header = None , columns = None , delimiter = \"::\" , filler = 0.0 , dtype_dict = None , ignored_columns = None , target_column = \"Rating\" , numerical_columns = None , categorical_columns = None , categorical_filter = 0 , fit_dictionary_path = None , transform_path = None , test_percentage = 0.1 , validate_percentage = 0.1 , train_path = None , validate_path = None , test_path = None , ) Preprocess the Movielens 1M dataset for rating prediction. Note To obtain the Movielens 1M dataset, visit: https://grouplens.org/datasets/movielens/ The Movielens 1M dataset has 4 data columns: UserID, MovieID, Rating, and Timestamp. Arguments non_csv_path (str) : Path to convert the dataset into CSV format. csv_path (str) : Path to save and load the CSV dataset. header (int) : Row number to use as column names. columns (list) : String names associated with the columns of the dataset. delimiter (str) : Separator used to parse lines. filler (float) : Filler value used to fill missing data. dtype_dict (dict) : Map string column names to column data type. ignored_columns (list) : String names associated with the columns to ignore. target_column (str) : String name associated with the columns containing target data for prediction, e.g., rating column for rating prediction and label column for click-through rate (CTR) prediction. numerical_columns (list) : String names associated with the columns containing numerical data. categorical_columns (list) : String names associated with the columns containing categorical data. categorical_filter (int) : Filter used to group infrequent categories in one column as the same category. fit_dictionary_path (str) : Path to the fit dictionary for categorical data. transform_path (str) : Path to the transformed dataset. test_percentage (float) : Percentage for the test set. validate_percentage (float) : Percentage for the validation set. train_path (str) : Path to the training set. validate_path (str) : Path to the validation set. test_path (str) : Path to the test set. [source]","title":"MovielensPreprocessor"},{"location":"preprocessor/#preprocess_4","text":"MovielensPreprocessor . preprocess () Apply all preprocessing steps to the Movielens 1M dataset. Returns 6-tuple of ndarray training input data, training output data, validation input data, validation output data, testing input data, and testing output data.","title":"preprocess"},{"location":"recommender/","text":"[source] RPRecommender autorecsys . recommender . RPRecommender ( ** kwargs ) A Rating-Prediction HyperModel based on connected Blocks and HyperBlocks. Arguments inputs : A list of input node(s) for the HyperGraph. outputs : A list of output node(s) for the HyperGraph. [source] CTRRecommender autorecsys . recommender . CTRRecommender ( ** kwargs ) A CTR-Prediction HyperModel based on connected Blocks and HyperBlocks. Arguments inputs : A list of input node(s) for the HyperGraph. outputs : A list of output node(s) for the HyperGraph.","title":"Recommender"},{"location":"recommender/#rprecommender","text":"autorecsys . recommender . RPRecommender ( ** kwargs ) A Rating-Prediction HyperModel based on connected Blocks and HyperBlocks. Arguments inputs : A list of input node(s) for the HyperGraph. outputs : A list of output node(s) for the HyperGraph. [source]","title":"RPRecommender"},{"location":"recommender/#ctrrecommender","text":"autorecsys . recommender . CTRRecommender ( ** kwargs ) A CTR-Prediction HyperModel based on connected Blocks and HyperBlocks. Arguments inputs : A list of input node(s) for the HyperGraph. outputs : A list of output node(s) for the HyperGraph.","title":"CTRRecommender"},{"location":"examples/ctr_autoint/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , SelfAttentionInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # Step 1: Preprocess data criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () train_X_numerical , train_X_categorical = criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X ) val_X_numerical , val_X_categorical = criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X ) test_X_numerical , test_X_categorical = criteo . get_x_numerical ( test_X ), criteo . get_x_categorical ( test_X ) numerical_count = criteo . get_numerical_count () categorical_count = criteo . get_categorical_count () hash_size = criteo . get_hash_size () # Step 2: Build the recommender, which provides search space dense_input_node = Input ( shape = [ numerical_count ]) sparse_input_node = Input ( shape = [ categorical_count ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = numerical_count , embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = categorical_count , hash_size = hash_size , embedding_dim = 2 )( sparse_input_node ) attention_output = SelfAttentionInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ attention_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # Step 3: Build the searcher, which provides search algorithm. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) # Step 4: Use the searcher to search the recommender searcher . search ( x = [ train_X_numerical , train_X_categorical ], y = train_y , x_val = [ val_X_numerical , val_X_categorical ], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) # Step 5: Evaluate the searched model logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ test_X_numerical , test_X_categorical ], y_true = val_y )))","title":"Ctr autoint"},{"location":"examples/ctr_autorec/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , HyperInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # Step 1: Preprocess data criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # Step 2: Build model dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) sparse_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_emb ]) dense_feat_bottom_output = HyperInteraction ( meta_interator_num = 2 )([ dense_feat_emb ]) hyper_output = HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_bottom_output , dense_feat_bottom_output ]) output = PointWiseOptimizer ()( hyper_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # Step 3: Build Searcher searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) # Step 4: Search model & HP searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr autorec"},{"location":"examples/ctr_benchmark/","text":"import argparse import time import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"1\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , DenseFeatureMapper , SparseFeatureMapper , \\ ElementwiseInteraction , FMInteraction , MLPInteraction , ConcatenateInteraction , \\ CrossNetInteraction , SelfAttentionInteraction , HyperInteraction , \\ PointWiseOptimizer from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.recommender import CTRRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) def build_dlrm ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] output = MLPInteraction ( num_layers = 2 )( emb_list ) else : sparse_feat_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] dense_feat_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( sparse_feat_mlp_output + dense_feat_mlp_output ) return output def build_deepfm ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ FMInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ FMInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_crossnet ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ CrossNetInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ CrossNetInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_autoint ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] fm_output = [ SelfAttentionInteraction ()( emb_list )] bottom_mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) else : fm_output = [ SelfAttentionInteraction ()( [ emb_dict [ 'sparse' ]] )] if 'sparse' in emb_dict else [] bottom_mlp_output = [ MLPInteraction ()( [ emb_dict [ 'dense' ]] )] if 'dense' in emb_dict else [] output = MLPInteraction ( num_layers = 2 )( fm_output + bottom_mlp_output ) return output def build_neumf ( emb_dict ): emb_list = [ emb for _ , emb in emb_dict . items ()] innerproduct_output = [ ElementwiseInteraction ( elementwise_type = \"innerporduct\" )( emb_list )] mlp_output = [ MLPInteraction ( num_layers = 2 )( emb_list )] output = innerproduct_output + mlp_output return output def build_autorec ( emb_dict ): if 'user' in emb_dict or 'item' in emb_dict : emb_list = [ emb for _ , emb in emb_dict . items ()] output = HyperInteraction ()( emb_list ) else : sparse_feat_bottom_output = [ HyperInteraction ( meta_interator_num = 2 )([ sparse_feat_emb ])] if 'sparse' in emb_dict else [] dense_feat_bottom_output = [ HyperInteraction ( meta_interator_num = 2 )([ dense_feat_emb ])] if 'dense' in emb_dict else [] top_mlp_output = HyperInteraction ( meta_interator_num = 2 )( sparse_feat_bottom_output + dense_feat_bottom_output ) output = HyperInteraction ( meta_interator_num = 2 )([ top_mlp_output ]) return output if __name__ == '__main__' : # parse args parser = argparse . ArgumentParser () parser . add_argument ( '-model' , type = str , help = 'input a model name' , default = 'dlrm' ) parser . add_argument ( '-data' , type = str , help = 'dataset name' , default = \"avazu\" ) parser . add_argument ( '-data_path' , type = str , help = 'dataset path' , default = './datasets/avazu/avazu_500K.txt' ) parser . add_argument ( '-sep' , type = str , help = 'dataset sep' ) parser . add_argument ( '-search' , type = str , help = 'input a search method name' , default = 'random' ) parser . add_argument ( '-batch_size' , type = int , help = 'batch size' , default = 256 ) parser . add_argument ( '-trials' , type = int , help = 'try number' , default = 10 ) parser . add_argument ( '-gpu_index' , type = int , help = 'the index of gpu to use' , default = 0 ) args = parser . parse_args () print ( \"args:\" , args ) os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = str ( args . gpu_index ) if args . sep == None : args . sep = '::' # Load and preprocess dataset if args . data == \"avazu\" : avazu = AvazuPreprocessor ( csv_path = args . data_path , validate_percentage = 0.1 , test_percentage = 0.1 ) train_X , train_y , val_X , val_y , test_X , test_y = avazu . preprocess () # dense_input_node = None sparse_input_node = Input ( shape = [ avazu . get_categorical_count ()]) input = [ sparse_input_node ] # dense_feat_emb = None sparse_feat_emb = SparseFeatureMapper ( num_of_fields = avazu . get_categorical_count (), hash_size = avazu . get_hash_size (), embedding_dim = 64 )( sparse_input_node ) emb_dict = { 'sparse' : sparse_feat_emb } if args . data == \"criteo\" : criteo = CriteoPreprocessor ( csv_path = args . data_path , validate_percentage = 0.1 , test_percentage = 0.1 ) train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_categorical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) input = [ dense_input_node , sparse_input_node ] dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 64 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 64 )( sparse_input_node ) emb_dict = { 'dense' : dense_feat_emb , 'sparse' : sparse_feat_emb } # select model if args . model == 'dlrm' : output = build_dlrm ( emb_dict ) if args . model == 'deepfm' : output = build_deepfm ( emb_dict ) if args . model == 'crossnet' : output = build_neumf ( emb_dict ) if args . model == 'autoint' : output = build_autorec ( emb_dict ) # if args.model == 'neumf': # output = build_autorec(emb_dict) if args . model == 'autorec' : output = build_autorec ( emb_dict ) output = PointWiseOptimizer ()( output ) model = CTRRecommender ( inputs = input , outputs = output ) # search and predict. searcher = Search ( model = model , tuner = args . search , tuner_params = { 'max_trials' : args . trials , 'overwrite' : True } ) start_time = time . time () searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = args . batch_size , epochs = 1 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) end_time = time . time () print ( \"running time:\" , end_time - start_time ) print ( \"args\" , args ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X [: 10 ]))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = test_X , y_true = test_y )))","title":"Ctr benchmark"},{"location":"examples/ctr_crossnet/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , CrossNetInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) crossnet_output = CrossNetInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ crossnet_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr crossnet"},{"location":"examples/ctr_deepfm/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) fm_output = FMInteraction ()([ sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ fm_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 2 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr deepfm"},{"location":"examples/ctr_deepfm_test_avazu/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.utils.common import set_seed # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) avazu = AvazuPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = avazu . preprocess () train_X = avazu . get_x_categorical ( train_X ) val_X = avazu . get_x_categorical ( val_X ) sparse_input_node = Input ( shape = [ len ( avazu . categorical_columns )]) # Step 1: Mapper sparse_feat_emb = SparseFeatureMapper ( num_of_fields = len ( avazu . categorical_columns ), hash_size = avazu . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) # Step 2.1: Interactor fm_output = FMInteraction ()([ sparse_feat_emb ]) # Step 2.2: Interactor top_mlp_output = MLPInteraction ()([ fm_output ]) # Step 3: Optimizer output = PointWiseOptimizer ()( top_mlp_output ) # Step 4: Recommender wrapper model = CTRRecommender ( inputs = [ sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True },) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1024 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting BinaryCrossentropy: {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y )))","title":"Ctr deepfm test avazu"},{"location":"examples/ctr_deepfm_test_criteo/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , FMInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor , AvazuPreprocessor from autorecsys.utils.common import set_seed # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) set_seed ( 0 ) st = time . time () criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () print ( \"Preprocessing time: \\t \" , time . time () - st ) # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) fm_output = FMInteraction ()([ sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ fm_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1024 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting BinaryCrossentropy: {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y ))) print ( \"Total time: \\t \" , time . time () - st )","title":"Ctr deepfm test criteo"},{"location":"examples/ctr_dlrm/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import tensorflow as tf import numpy as np from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. dense_input_node = Input ( shape = [ criteo . get_numerical_count ()]) sparse_input_node = Input ( shape = [ criteo . get_categorical_count ()]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . get_numerical_count (), embedding_dim = 2 )( dense_input_node ) sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . get_categorical_count (), hash_size = criteo . get_hash_size (), embedding_dim = 2 )( sparse_input_node ) sparse_feat_mlp_output = MLPInteraction ()([ sparse_feat_emb ]) dense_feat_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ( num_layers = 2 )([ sparse_feat_mlp_output , dense_feat_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_numerical ( train_X ), criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 10000 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )])[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_numerical ( val_X ), criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr dlrm"},{"location":"examples/ctr_neumf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , PointWiseOptimizer , ElementwiseInteraction from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo = CriteoPreprocessor () # automatically set up for preprocessing the Criteo dataset train_X , train_y , val_X , val_y , test_X , test_y = criteo . preprocess () # build the pipeline. input = Input ( shape = [ criteo . get_categorical_count ()]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = 10000 , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = 10000 , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = PointWiseOptimizer ()([ innerproduct_output , mlp_output ]) model = CTRRecommender ( inputs = input , outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 10 , 'overwrite' : True }, ) searcher . search ( x = [ criteo . get_x_categorical ( train_X )], y = train_y , x_val = [ criteo . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 256 , epochs = 20 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )] ) logger . info ( 'Predicted Ratings: {} ' . format ( searcher . predict ( x = [ criteo . get_x_categorical ( val_X )]))) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = [ criteo . get_x_categorical ( val_X )], y_true = val_y )))","title":"Ctr neumf"},{"location":"examples/ctr_test_criteo/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"7\" import logging import numpy as np import time from autorecsys.auto_search import Search from autorecsys.pipeline import Input , DenseFeatureMapper , SparseFeatureMapper , SelfAttentionInteraction , MLPInteraction , PointWiseOptimizer from autorecsys.recommender import CTRRecommender from autorecsys.pipeline.preprocessor import CriteoPreprocessor st = time . time () # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset criteo_path = \"./examples/datasets/criteo_full/train.txt\" criteo = CriteoPreprocessor ( criteo_path ) criteo . preprocessing ( test_size = 0.2 ) train_X , train_y , val_X , val_y = criteo . train_X , criteo . train_y , criteo . val_X , criteo . val_y # TODO: preprocess train val split # build the pipeline. dense_input_node = Input ( shape = [ criteo . numer_num ]) sparse_input_node = Input ( shape = [ criteo . categ_num ]) dense_feat_emb = DenseFeatureMapper ( num_of_fields = criteo . numer_num , embedding_dim = 2 )( dense_input_node ) # TODO: preprocess data to get sparse hash_size sparse_feat_emb = SparseFeatureMapper ( num_of_fields = criteo . categ_num , hash_size = criteo . hash_sizes , embedding_dim = 2 )( sparse_input_node ) attention_output = SelfAttentionInteraction ()([ dense_feat_emb , sparse_feat_emb ]) bottom_mlp_output = MLPInteraction ()([ dense_feat_emb ]) top_mlp_output = MLPInteraction ()([ attention_output , bottom_mlp_output ]) output = PointWiseOptimizer ()( top_mlp_output ) model = CTRRecommender ( inputs = [ dense_input_node , sparse_input_node ], outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , tuner_params = { 'max_trials' : 2 , 'overwrite' : True }, ) searcher . search ( x = train_X , y = train_y , x_val = val_X , y_val = val_y , objective = 'val_BinaryCrossentropy' , batch_size = 1000 ) logger . info ( 'First 10 Predicted Ratings: {} ' . format ( searcher . predict ( x = val_X )[: 10 ])) logger . info ( 'Predicting Accuracy (logloss): {} ' . format ( searcher . evaluate ( x = val_X , y_true = val_y ))) print ( time . time () - st )","title":"Ctr test criteo"},{"location":"examples/netflix/","text":"import time begin = time . time () netflix = NetflixPrizePreprocessor () netflix . preprocess () print ( time . time () - begin )","title":"Netflix"},{"location":"examples/rp_autorec/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"2\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , HyperInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () user_num , item_num = movielens . get_hash_size () # build the pipeline. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output1 = HyperInteraction ()([ user_emb , item_emb ]) output2 = HyperInteraction ()([ output1 , user_emb , item_emb ]) output3 = HyperInteraction ()([ output1 , output2 , user_emb , item_emb ]) output4 = HyperInteraction ()([ output1 , output2 , output3 , user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output4 ) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict. searcher = Search ( model = model , tuner = 'random' , ## hyperband, bayesian tuner_params = { 'max_trials' : 2 , 'overwrite' : True },) searcher . search ( x = [ movielens . get_x_categorical ( train_X )], y = train_y , x_val = [ movielens . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 1 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( val_X ), y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( test_X ), y_true = test_y )))","title":"Rp autorec"},{"location":"examples/rp_benchmark/","text":"import argparse import time import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"5\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , HyperInteraction , MLPInteraction , \\ ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor , NetflixPrizePreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) def build_mf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_gmf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_mlp ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = user_num , embedding_dim = 64 )( input ) output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model def build_neumf ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()([ innerproduct_output , mlp_output ]) model = RPRecommender ( inputs = input , outputs = output ) return model def build_autorec ( user_num , item_num ): input = Input ( shape = [ 2 ]) user_emb_1 = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_1 = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) user_emb_2 = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_2 = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = HyperInteraction ()([ user_emb_1 , item_emb_1 , user_emb_2 , item_emb_2 ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) return model if __name__ == '__main__' : # parse args parser = argparse . ArgumentParser () parser . add_argument ( '-model' , type = str , help = 'input a model name' ) parser . add_argument ( '-data' , type = str , help = 'dataset name' ) parser . add_argument ( '-data_path' , type = str , help = 'dataset path' ) parser . add_argument ( '-sep' , type = str , help = 'dataset sep' ) parser . add_argument ( '-search' , type = str , help = 'input a search method name' ) parser . add_argument ( '-batch_size' , type = int , help = 'batch size' ) parser . add_argument ( '-epochs' , type = int , help = 'epochs' ) parser . add_argument ( '-early_stop' , type = int , help = 'early stop' ) parser . add_argument ( '-trials' , type = int , help = 'try number' ) args = parser . parse_args () # print(\"args:\", args) if args . sep == None : args . sep = '::' # Load dataset if args . data == \"ml\" : data = MovielensPreprocessor ( csv_path = args . data_path , validate_percentage = 0.1 , test_percentage = 0.1 ) train_X , train_y , val_X , val_y , test_X , test_y = data . preprocess () user_num , item_num = data . get_hash_size () # if args.data == \"netflix\": # dataset_paths = [args.data_path + \"/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) logger . info ( 'train_X size: {} ' . format ( train_X . shape )) logger . info ( 'train_y size: {} ' . format ( train_y . shape )) logger . info ( 'val_X size: {} ' . format ( val_X . shape )) logger . info ( 'val_y size: {} ' . format ( val_y . shape )) logger . info ( 'test_X size: {} ' . format ( test_X . shape )) logger . info ( 'test_y size: {} ' . format ( test_y . shape )) logger . info ( 'user total number: {} ' . format ( user_num )) logger . info ( 'item total number: {} ' . format ( item_num )) # select model if args . model == 'mf' : model = build_mf ( user_num , item_num ) if args . model == 'mlp' : model = build_mlp ( user_num , item_num ) if args . model == 'gmf' : model = build_gmf ( user_num , item_num ) if args . model == 'neumf' : model = build_neumf ( user_num , item_num ) if args . model == 'autorec' : model = build_autorec ( user_num , item_num ) # search and predict. searcher = Search ( model = model , tuner = args . search , tuner_params = { 'max_trials' : args . trials , 'overwrite' : True } ) start_time = time . time () searcher . search ( x = data . get_x_categorical ( train_X ), y = train_y , x_val = data . get_x_categorical ( val_X ), y_val = val_y , objective = 'val_mse' , batch_size = args . batch_size , epochs = args . epochs , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = args . early_stop )]) end_time = time . time () # print(\"Runing time:\", end_time - start_time) # print(\"Args\", args) logger . info ( 'Runing time: {} ' . format ( end_time - start_time )) logger . info ( 'Args: {} ' . format ( args )) logger . info ( 'Predicting Accuracy (mse): {} ' . format ( searcher . evaluate ( x = data . get_x_categorical ( test_X ), y_true = test_y )))","title":"Rp benchmark"},{"location":"examples/rp_mf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"5\" import tensorflow as tf # gpus = tf.config.experimental.list_physical_devices(device_type='GPU') # for gpu in gpus: # tf.config.experimental.set_memory_growth(gpu, True) # import tensorflow as tf # physical_devices = tf.config.list_physical_devices('GPU') # tf.config.experimental.set_memory_growth(physical_devices[0], True) import logging from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , RatingPredictionOptimizer , ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor , NetflixPrizePreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset movielens = NetflixPrizePreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () user_num , item_num = movielens . get_hash_size () # build the pipeline. input = Input ( shape = [ 2 ]) user_emb = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb , item_emb ]) output = RatingPredictionOptimizer ()( output ) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict searcher = Search ( model = model , tuner = 'greedy' , # hyperband, greedy, bayesian tuner_params = { \"max_trials\" : 5 } ) searcher . search ( x = [ movielens . get_x_categorical ( train_X )], y = train_y , x_val = [ movielens . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 10 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( val_X ), y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( test_X ), y_true = test_y )))","title":"Rp mf"},{"location":"examples/rp_neumf/","text":"import os os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"6\" import logging import tensorflow as tf from autorecsys.auto_search import Search from autorecsys.pipeline import Input , LatentFactorMapper , MLPInteraction , RatingPredictionOptimizer , \\ ElementwiseInteraction from autorecsys.pipeline.preprocessor import MovielensPreprocessor from autorecsys.recommender import RPRecommender # logging setting logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logging . basicConfig ( level = logging . DEBUG , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' ) logger = logging . getLogger ( __name__ ) # load dataset ##Netflix Dataset # dataset_paths = [\"./examples/datasets/netflix-prize-data/combined_data_\" + str(i) + \".txt\" for i in range(1, 5)] # data = NetflixPrizePreprocessor(dataset_paths) #Movielens 1M Dataset movielens = MovielensPreprocessor () train_X , train_y , val_X , val_y , test_X , test_y = movielens . preprocess () user_num , item_num = movielens . get_hash_size () # build the pipeline. input = Input ( shape = [ 2 ]) user_emb_gmf = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_gmf = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) innerproduct_output = ElementwiseInteraction ( elementwise_type = \"innerporduct\" )([ user_emb_gmf , item_emb_gmf ]) user_emb_mlp = LatentFactorMapper ( feat_column_id = 0 , id_num = user_num , embedding_dim = 64 )( input ) item_emb_mlp = LatentFactorMapper ( feat_column_id = 1 , id_num = item_num , embedding_dim = 64 )( input ) mlp_output = MLPInteraction ()([ user_emb_mlp , item_emb_mlp ]) output = RatingPredictionOptimizer ()([ innerproduct_output , mlp_output ]) model = RPRecommender ( inputs = input , outputs = output ) # AutoML search and predict searcher = Search ( model = model , tuner = 'greedy' , # random, greedy tuner_params = { \"max_trials\" : 5 , 'overwrite' : True } ) searcher . search ( x = [ movielens . get_x_categorical ( train_X )], y = train_y , x_val = [ movielens . get_x_categorical ( val_X )], y_val = val_y , objective = 'val_mse' , batch_size = 1024 , epochs = 1 , callbacks = [ tf . keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 1 )]) logger . info ( 'Predicting Val Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( val_X ), y_true = val_y ))) logger . info ( 'Predicting Test Dataset Accuracy (mse): {} ' . format ( searcher . evaluate ( x = movielens . get_x_categorical ( test_X ), y_true = test_y )))","title":"Rp neumf"}]}